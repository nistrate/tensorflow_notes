{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGEl+LRV4GKf4hiDB726ic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nistrate/tensorflow_notes/blob/master/Artificial_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl1w7NW3yEFm",
        "colab_type": "text"
      },
      "source": [
        "This section will introduce thw concept of a *feed forward neural network*.\n",
        "\n",
        "Such a network is the basisi for other kinds of more complex networks such as\n",
        "- Convolutional Neural Networks (CNNs)\n",
        "- Recurrent Neural Networks (RNNs)\n",
        "\n",
        "We are trying to createa neural network in a computer. The name derives from the brain neurons. An artificial neural network is trying to simulate a working brain, or more precise a collection of neurons with their chemical and electrical connectivity.\n",
        "\n",
        "A feed forward network is formed of multiple interconnected layers of neurons. The input is fed in the first layer, while the output comes out of the last layer.\n",
        "\n",
        "Section outline:\n",
        "- Model Architecture.\n",
        "  - How does a feedforward neural network work?\n",
        "  - What is its ``equation\"?\n",
        "- The geometric picture.\n",
        "  - How does this relate back to the ``machine learning is nothing but a geometry problem''?\n",
        "- Activation functions.\n",
        "  - These are what make neural network more powerful.\n",
        "- Multiclass clasification.\n",
        "  - Previously only discussed binary classifications (dog/cat, fraud/no fraud, pruchase/leave).\n",
        "- Image data\n",
        "  - Deep learning excels at images, text, and sound.\n",
        "  - How does this relate to ``all data is the same\"?\n",
        "- Image classification notebook; Regression notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA43V4EmsaC",
        "colab_type": "text"
      },
      "source": [
        "Forward Propagation\n",
        "\n",
        "Consider a dense network of Input $\\rightarrow$ Layer One $\\rightarrow$ Ouput\n",
        "\n",
        "The connection between the input to a node in Layer 1 is logistic regression and represents a neuron. Each neuron may be calculationg something different, given a different set of weights.\n",
        "\n",
        "E.g. input is a face. One neuron looks for the presence of a an eye, another looks for the presence of a nose. We say that each neuron is looking for a certain feature.\n",
        "\n",
        "1) The same input can be fed to multiple diffferent neurons, each calculating somethiong different (more neurons per layer).\n",
        "\n",
        "2) Neurons in one layer ca act as inputs to another layer.\n",
        "\n",
        "Lines $\\rightarrow$ Neurons\n",
        "\n",
        " - a line $ ax+b $\n",
        " - a neuron $\\sigma(w^{T}x+b)$\n",
        "\n",
        "The natural question arrises: What should we do if we have more than one neuron per layer?\n",
        "\n",
        "- call the output of the $j$th neuron - $z_j$\n",
        "- say there are $M$ neurons, so $j=1,2 ,...,M$\n",
        "\n",
        "$$\n",
        "z_j = \\sigma(w_j^T x + b_j) ~~~for~~~ j = 1,2,..., M \n",
        "$$\n",
        "or more compactly\n",
        "$$\n",
        "z = \\sigma(W^T x + b)\n",
        "$$\n",
        "where\n",
        "- $z$ is a vector of size $M \\times 1$\n",
        "- $x$ is a vector of size $D \\times 1$\n",
        "- $W$ is a matrix of size $D \\times M$\n",
        "- $b$ is a vector of size $M \\times 1$\n",
        "- $\\sigma()$ is an element wise operation\n",
        "\n",
        "thus \n",
        "\n",
        "$$\n",
        "p(y=1|x) = \\sigma(W^{(L)~T~}z^{(L-1)} + b^{(L)})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae4YWQIJhWND",
        "colab_type": "text"
      },
      "source": [
        "Activation Functions\n",
        "\n",
        "The sigmoid function\n",
        "- maps values to $(0...1)$ - which mimics the biological neuron\n",
        "- practical use: it makes the neural network's decision boundary nonlinear\n",
        "\n",
        "Unfortunately, there are some problems with the sigmoid\n",
        "\n",
        "$$\n",
        "\\sigma(a) = \\frac{1}{1+\\exp{(-a)}}\n",
        "$$\n",
        "\n",
        "Standadization\n",
        "\n",
        "- we don't want to havet one input in the range $1 .. 5$ million, or another in the range $0 .. 0.0001$.\n",
        "- we prefer inputs to be centered around $0$ and approx. in the same range.\n",
        "- the sigmoid output goes between $0$ and $1$, which means that it's centered around $0.5$\n",
        "  - it's ouptput therefore cannot be centered around $0$\n",
        "- recall the concept of ``uniformity''\n",
        "  - The output of the sigmoid (previous layer) is the input to the next layer\n",
        "- We want bot the input and output to be standardized\n",
        "\n",
        "The solution is to use a hyperbolic tangent $\\tanh$\n",
        "\n",
        "- $\\tanh$ is a bit better than the sigmoid, but both are still problematic\n",
        "consider the cautionary tale\n",
        "- there's a ``beauty'' in the neural network as-is (unifrom, idealized neurons)\n",
        "- sigmoid and $\\tanh$ are smooth, differentiable functions\n",
        "- Nobody wants to break from the ``classic'' way of doing things\n",
        "\n",
        "The major problem with these activation function is due to the *vanishing gradient problem* ,  and it appears due to the gradient descent way of training the network.\n",
        "\n",
        "A solutioon to this issue is to use activation functions that do not have a vanishing gradient. One of these functiion is *ReLU* (rectifier linear unit)\n",
        "$$\n",
        "R(z) = max(0,z)\n",
        "$$\n",
        "\n",
        "However, this comes with it's own issues - the *dead neuron problem*\n",
        "an alternative is the *LReLU* (Leacky ReLU) or the *ELU* (exponential linear unit), or *Softplus*.\n",
        "\n",
        "Biological Plausability\n",
        "- ReLu may be even a more biologically plausible than sigmoid, but requires a different perspective.\n",
        "- Action Potentials by themselves are not distinct wrt stimuli\n",
        "- Instead, it is the frequency of action potentials that changes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obMUH6Hjn6Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}